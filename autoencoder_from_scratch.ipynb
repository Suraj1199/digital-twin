{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autoencoder Tutorial\n",
    "## Complete Implementation from Scratch\n",
    "\n",
    "This notebook covers:\n",
    "- Basic Fully Connected Autoencoder\n",
    "- Convolutional Autoencoder\n",
    "- Variational Autoencoder (VAE)\n",
    "- Training and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "latent_dim = 32\n",
    "input_dim = 784  # 28x28 MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Basic Fully Connected Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Simple fully connected autoencoder for MNIST\"\"\"\n",
    "    def __init__(self, input_dim=784, latent_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "print('Basic Autoencoder defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Convolutional Autoencoder\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    \"\"\"Convolutional autoencoder for image data\"\"\"\n",
    "    def __init__(self, channels=1):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "print('Convolutional Autoencoder defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Variational Autoencoder (VAE)\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder with reparameterization trick\"\"\"\n",
    "    def __init__(self, input_dim=784, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, 400)\n",
    "        self.fc4 = nn.Linear(400, input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"VAE loss = Reconstruction loss + KL divergence\"\"\"\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "print('VAE defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load MNIST Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Functions\n",
    "def train_autoencoder(model, train_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            images_flat = images.view(images.size(0), -1)\n",
    "            \n",
    "            outputs = model(images_flat)\n",
    "            loss = criterion(outputs, images_flat)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_vae(model, train_loader, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            images_flat = images.view(images.size(0), -1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(images_flat)\n",
    "            loss = vae_loss(recon_batch, images_flat, mu, logvar)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item()/len(images):.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print('Training functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize and Train Model\n",
    "# Choose model type: 'basic', 'conv', or 'vae'\n",
    "model_type = 'basic'  # Change this to experiment\n",
    "\n",
    "if model_type == 'basic':\n",
    "    model = Autoencoder(input_dim=784, latent_dim=latent_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = train_autoencoder(model, train_loader, criterion, optimizer, epochs, device)\n",
    "    \n",
    "elif model_type == 'vae':\n",
    "    model = VAE(input_dim=784, latent_dim=20)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = train_vae(model, train_loader, optimizer, epochs, device)\n",
    "    \n",
    "else:\n",
    "    print('For ConvAutoencoder, use 2D images instead of flattened')\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization Functions\n",
    "def visualize_reconstructions(model, dataloader, device, num_images=10, model_type='basic'):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images[:num_images].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images_flat = images.view(images.size(0), -1)\n",
    "        if model_type == 'vae':\n",
    "            reconstructed, _, _ = model(images_flat)\n",
    "        else:\n",
    "            reconstructed = model(images_flat)\n",
    "        reconstructed = reconstructed.view(-1, 1, 28, 28)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
    "    for i in range(num_images):\n",
    "        axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', size=14)\n",
    "    axes[1, 0].set_ylabel('Reconstructed', size=14)\n",
    "    plt.suptitle('Autoencoder Reconstructions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_loss(losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training Loss over Time', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print('Visualization functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Results\n",
    "plot_training_loss(losses)\n",
    "visualize_reconstructions(model, test_loader, device, num_images=10, model_type=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Model\n",
    "model_path = f'{model_type}_autoencoder.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Generate Samples (VAE only)\n",
    "if model_type == 'vae':\n",
    "    def generate_samples(model, num_samples=10, device='cpu'):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, model.fc3.in_features).to(device)\n",
    "            generated = model.decode(z)\n",
    "            generated = generated.view(-1, 1, 28, 28)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(15, 2))\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(generated[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "        plt.suptitle('Generated Samples from VAE', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    generate_samples(model, num_samples=10, device=device)\n",
    "else:\n",
    "    print('Sample generation only available for VAE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
